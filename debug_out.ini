这个输出是从循环后退出来的,因此是15个线程中最后一个线程的输出,,因此seed=127,并且evaluator是seed=127的最后一种配置,GCG
print(cfg.meta)
device='cuda' use_neptune=True 
neptune_project='wz-2024/watermark' 
seed=127 out_root_dir='out_llama13b/' result_dir='results/default'

print(cfg.server)
model=ModelConfig(skip=True, name='/data_disk/dyy/models/meta-llama/Llama-2-13b-chat-hf',
             use_fp16=True, use_flashattn2=True, prompt_max_len=800, response_max_len=800, 
             n_beams=1, use_sampling=True, sampling_temp=0.7
            ) 
watermark=WatermarkConfig(scheme=<WatermarkScheme.KGW: 'kgw'>, 
                        generation=WatermarkGenerationConfig(seeding_scheme='selfhash', 
                        gamma=0.25, delta=4.0), 
                        detection=WatermarkDetectionConfig(normalizers=[], 
                                                            ignore_repeated_ngrams=True, 
                                                            z_threshold=4.0
                                                        )
                        ) 
disable_watermark=False

print(cfg.attacker)
algo=<AttackerAlgo.OUR: 'our'> 
model=ModelConfig(skip=False, name='mistralai/Mistral-7B-Instruct-v0.1', use_fp16=True, 
                    use_flashattn2=True, prompt_max_len=800, response_max_len=800, 
                    n_beams=1, use_sampling=True, sampling_temp=0.7) 
querying=AttackerQueryingConfig(skip=True, dataset='c4', batch_size=64, 
                                start_from_batch_num=0, end_at_batch_num=500, apply_watermark=True) 
learning=AttackerLearningConfig(skip=False, mode=<AttackerLearningMode.FAST: 'fast'>, 
                                nb_queries=30000) 
generation=AttackerGenerationConfig(spoofer_strength=6.5, w_abcd=2.0, w_partials=1.0, 
                                    w_empty=0.5, w_future=0.0, min_wm_count_nonempty=2, 
                                    min_wm_mass_empty=7e-05, future_num_cands=5, 
                                    future_num_options_per_fillin=10, future_prefix_size=10, 
                                    future_local_w_decay=0.9, panic_from=750, repetition_penalty=1.6, 
                                    use_ft_counts=True, use_graceful_conclusion=True, 
                                    sysprompt=<SyspromptType.STANDARD: 'standard'>, dipper_chunk=3, 
                                    dipper_lexdiv=60, dipper_orderdiv=20, recursive_iters=1, 
                                    prevent_eos_if_zest_bad=True, clip_at=2.0
                                )

print(cfg.evaluator)
skip=False 
get_server_prompts_from=None 
run_baseline_only=False 
batch_size=8 
metrics=[<EvalMetric.DETECTOR: 'detector'>, <EvalMetric.PPL: 'ppl'>, 
            <EvalMetric.GPT4: 'gpt4'>, <EvalMetric.SELF: 'self'>]
eval_class=<EvalClass.SPOOFING: 'spoofing'> 
eval_mode=<EvalMode.TGT_GCG: 'gcg-advbench-50'> start_from_idx=-1

## 参考文献：<a href="http://arxiv.org/abs/2402.19361">Watermark Stealing in LLM</a>
本repo的大部分代码来自上述文献，这里说明一些代码运行中可能存在的一些问题，以及对执行流和代码结构的解释.
## 1.环境搭建
这里请读者参考原仓库提供的说明文档。需要说明的是:<br>
1.`xform`可以不装。<br>
2.原作者在`bash`文件中写入了这个`pip install -U flash-attn --no-build-isolation`。但是这个库直接这样下载是不太可能成功的，您需要下载预编译`whl`文件，然后把它安装到当前环境中。<br>
3.由于某些原因，直接使用GPT官方的API使用方法很可能连不通，这时可以使用转发接口，具体的调用函数在本repo已经做好了修改.接口租赁可以参考某橙色软件，价格大概是250r/每次完整实验。（该实验指的是窃取实验`python3 main.py configs/spoofing/llama13b/mistral_selfhash.yaml`)
4.在result中放的是我跑出来的若干结果,GPU环境是A100,时间<20h。默认配置下的评价指标都有，包括GPT_Score,开销大概是350r。<br>
5.原项目使用了`accelerate`加速,但是作者的`accelerate`写的非常一般,完全没考虑到单卡显存不够用的情况,这里列举您运行时可能存在的问题以及几种解决方案:<br>
a.如果您使用的服务器是多卡环境,很可能出现这个错误:"多个tensor不在同一个`cuda`上"这很可能是因为原作者配置`accelerate`时候,他们使用的是单卡(显存较大)环境,但是默认配置为了多卡环境,这时您有两个选择<br>
第一,您可以直接大规模修改代码中`accelerate`对于`cuda`的使用,让它适应多卡环境<br>
第二,如果您单卡的显存够用(30G左右),可以在环境中设置当前仅一张卡可见,这样即便`accelerate`默认配置为多卡,最终也会使用单卡<br>
b.在运行水印擦除时,由于使用的是`dipper`这个模型,显存占用可能比较大,不做任何优化可能占用46G左右的显存,这时您可以使用`pf16`版本的模型,显存占用会有非常明显的下降。经过实验我发现是否使用`fp16`对最终结果完全没影响。<br>
c.在完成水印擦除任务时,您可以使用其他`T5-based`LLM,这一点在文章中是没有体现的,但是在代码中有所体现。笔者猜测具体的原因可能是,对于擦除任务来讲,模型并不需要很好的生成能力,而是理解/改写能力,而`T5`有较好的理解能力,所以可以使用它。
## 2.执行流详解
这里请参考笔者的这份文档：
https://fcn67dvu0kd4.feishu.cn/docx/GPQUdYViMoWbIMxWBZYcYiqVnWF?from=from_copylink <br>
我从文章的理解开始，一直讲到运行得到的最终效果，非常全面
## 3.讨论
首先说明，这篇文献作为`ICML-2024`，提出的方法非常牛，时间开销和计算资源开销都很小(video memory<30G,<20h).<br>
并且提出了水印窃取和水印擦除并没有存在权衡的关系。利用`dipper`模型后，擦除效果更是远高于之前的SOTA。<br>
笔者认为一点小缺陷是远项目的代码组织有点混乱,`main.py`中的`Evaluator`中还包含了许多非`Evaluate`的逻辑.调用关系比较混乱，有些设计模式也是乱用,很难评价.在上述的执行流文档中理清了关系，但是耗费了好几天的时间。

meta:
  device: 'cuda'
  use_neptune: true
  seed: [123, 124, 125, 126, 127]
  out_root_dir: 'out_llama13b/'
  result_dir: 'results/scrubbing_self_hash'
server:
  model:
    cfg_path: 'configs/model/llama13b.yaml'
    skip: true
    response_max_len: 1600 # important if generating! 
  watermark:
    scheme: 'kgw'
    generation:
      seeding_scheme: 'ff-additive_prf-3-False-15485863'
      gamma: 0.25
      delta: 4.0 
    detection:
      normalizers: []
      ignore_repeated_ngrams: true
      z_threshold: 4.0 
attacker:
  algo: 'our'
  model:
    cfg_path: 'configs/model/dipper.yaml'
    skip: false
    prompt_max_len: 5000
  querying:
    skip: true
    dataset: 'c4'
    batch_size: 64
    start_from_batch_num: 0
    apply_watermark: true 
  learning:
    skip: false
    mode: 'fast' 
    nb_queries: 30000
  generation:
    dipper_lexdiv: 60
    dipper_orderdiv: 20 
    dipper_chunk: 3
    repetition_penalty: 1.6
    spoofer_strength: -10
    use_ft_counts: true
    w_abcd: 2.0
    w_partials: 1.0
    w_empty: 0.5
    w_future: 0.0
    min_wm_count_nonempty: 2 
    min_wm_mass_empty: 0.00007
    future_num_cands: 5
    future_num_options_per_fillin: 10
    future_prefix_size: 10 
    future_local_w_decay: 0.9 
    prevent_eos_if_zest_bad: false
    use_graceful_conclusion: false 
    panic_from: 750
evaluator:
  skip: false
  get_server_prompts_from: "WS-1498"
  metrics:
    - "detector"
    - "psp"
  eval_class: "scrubbing"
  eval_mode: ["dolly-writing-100-long"]
gradio:
  skip: true 
  make_public: false 
  port: 7860
  default_prompt: "Write five sentences about war. "
